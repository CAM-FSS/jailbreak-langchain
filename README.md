In this paper, we conduct the first work to propose the concept of indirect jailbreak and achieve Retrieval-Augmented Generation (RAG) via LangChain. Building on this, we further design a novel method of indirect jailbreak attack, termed Poisoned-LangChain (PLC), which leverages a poisoned external knowledge base to interact with large language models, thereby causing the large models to generate malicious non-compliant dialogues.We tested this method on six different large language models across three major categories of jailbreak issues. The experiments demonstrate that PLC successfully implemented indirect jailbreak attacks under three different scenarios, achieving success rates of 88.56%, 79.04%, and 82.69% 
#![overall_00(1)](https://github.com/CAM-FSS/jailbreak-langchain/assets/122528037/7507a180-ef53-41f0-86a3-e06086fa3ea2)
#
![效果图](https://github.com/CAM-FSS/jailbreak-langchain/assets/122528037/91e0903e-2f3f-4e3c-bc62-892b74c93888)


